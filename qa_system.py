import os
from openai import OpenAI
from langchain_community.document_loaders import (
    TextLoader,
    CSVLoader,
    UnstructuredPDFLoader,
    Docx2txtLoader,
    UnstructuredExcelLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import Document
from typing import List, Optional
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class QASystemBuilder:
    def __init__(self):
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = OpenAI(
            base_url="https://ark.cn-beijing.volces.com/api/v3",
            api_key=os.environ.get("ARK_API_KEY"),
        )
        
        # åˆå§‹åŒ–LangChainç»„ä»¶
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=os.environ.get("ARK_API_KEY"),
            openai_api_base="https://ark.cn-beijing.volces.com/api/v3"
        )
        
    def load_documents(self, file_path: str) -> List[Document]:
        """åŠ è½½æœ¬åœ°æ–‡æ¡£"""
        if file_path.endswith('.txt'):
            loader = TextLoader(file_path)
        elif file_path.endswith('.csv'):
            loader = CSVLoader(file_path)
        elif file_path.endswith('.pdf'):
            loader = UnstructuredPDFLoader(file_path)
        elif file_path.endswith('.docx'):
            loader = Docx2txtLoader(file_path)
        elif file_path.endswith('.xlsx'):
            loader = UnstructuredExcelLoader(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡æ¡£æ ¼å¼: {file_path}")
        
        return loader.load()
    
    def process_documents(self, docs: List[Document], persist_dir: str = "./chroma_db") -> Chroma:
        """å¤„ç†æ–‡æ¡£å¹¶åˆ›å»ºå‘é‡æ•°æ®åº“"""
        # æ–‡æ¡£åˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(docs)
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        vectordb = Chroma.from_documents(
            documents=splits,
            embedding=self.embeddings,
            persist_directory=persist_dir
        )
        vectordb.persist()
        return vectordb
    
    def create_qa_chain(self, vectordb: Chroma, model_name: str = "doubao-pro-32k-241215") -> RetrievalQA:
        """åˆ›å»ºé—®ç­”é“¾"""
        llm = ChatOpenAI(
            model_name=model_name,
            temperature=0,
            openai_api_key=os.environ.get("ARK_API_KEY"),
            openai_api_base="https://ark.cn-beijing.volces.com/api/v3"
        )
        
        return RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectordb.as_retriever(search_kwargs={"k": 3}),
            return_source_documents=True
        )
    
    def query(self, qa_chain: RetrievalQA, question: str, stream: bool = False) -> None:
        """æŸ¥è¯¢é—®ç­”ç³»ç»Ÿ"""
        if stream:
            # æµå¼å“åº” - ä½¿ç”¨çœŸæ­£çš„æµå¼è¾“å‡º
            print("ğŸ¤– AIå›ç­”: ", end="", flush=True)
            
            try:
                # è·å–ç›¸å…³æ–‡æ¡£
                retriever = qa_chain.retriever
                docs = retriever.get_relevant_documents(question)
                
                # æ„å»ºä¸Šä¸‹æ–‡
                context = "\n\n".join([doc.page_content for doc in docs])
                
                # æ„å»ºæç¤ºè¯
                prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œè¦æ±‚å‡†ç¡®ã€è¯¦ç»†ã€‚"""
                
                # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è¿›è¡Œæµå¼è°ƒç”¨
                response = self.client.chat.completions.create(
                    model="doubao-pro-32k-241215",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯é—®ç­”åŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    stream=True
                )
                
                # é€å­—æ˜¾ç¤ºå“åº”
                full_response = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        print(content, end="", flush=True)
                        full_response += content
                
                print()  # æ¢è¡Œ
                print("âœ… æµå¼å›ç­”å®Œæˆ")
                
                # ä¿å­˜ç»“æœç”¨äºæ˜¾ç¤ºæ¥æºæ–‡æ¡£
                result = {"result": full_response, "source_documents": docs}
                
            except Exception as e:
                print(f"\nâŒ æµå¼å›ç­”å‡ºé”™: {e}")
                return
                
        else:
            # æ ‡å‡†å“åº”
            print("ğŸ¤– AIå›ç­”: ", end="", flush=True)
            try:
                result = qa_chain({"query": question})
                print(result["result"])
                print("âœ… æ ‡å‡†å›ç­”å®Œæˆ")
            except Exception as e:
                print(f"\nâŒ æ ‡å‡†å›ç­”å‡ºé”™: {e}")
                return
        
        # æ˜¾ç¤ºæ¥æºæ–‡æ¡£
        if 'source_documents' in result:
            print("\nğŸ“š æ¥æºæ–‡æ¡£:")
            for i, doc in enumerate(result["source_documents"], 1):
                print(f"\nã€æ–‡æ¡£ {i}ã€‘")
                print(f"å†…å®¹: {doc.page_content[:200]}...")
                print(f"æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}")
        else:
            print("\nğŸ“š æœªæ‰¾åˆ°æ¥æºæ–‡æ¡£")

def main():
    # åˆå§‹åŒ–ç³»ç»Ÿæ„å»ºå™¨
    builder = QASystemBuilder()
    
    # 1. åŠ è½½æœ¬åœ°è¯æœ¯åº“
    file_path = input("è¯·è¾“å…¥è¯æœ¯åº“æ–‡ä»¶è·¯å¾„: ")
    try:
        docs = builder.load_documents(file_path)
        print(f"æˆåŠŸåŠ è½½æ–‡æ¡£: {len(docs)}ä¸ªæ®µè½")
    except Exception as e:
        print(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
        return
    
    # 2. å¤„ç†æ–‡æ¡£å¹¶åˆ›å»ºå‘é‡æ•°æ®åº“
    persist_dir = "./chroma_db"
    vectordb = builder.process_documents(docs, persist_dir)
    print(f"å‘é‡æ•°æ®åº“å·²åˆ›å»ºå¹¶ä¿å­˜åˆ°: {persist_dir}")
    
    # 3. åˆ›å»ºé—®ç­”é“¾
    qa_chain = builder.create_qa_chain(vectordb)
    print("é—®ç­”ç³»ç»Ÿå·²å°±ç»ªï¼Œå¼€å§‹äº¤äº’...\n")
    
    # 4. äº¤äº’é—®ç­”
    while True:
        question = input("\nè¯·è¾“å…¥é—®é¢˜(è¾“å…¥'é€€å‡º'ç»“æŸ): ")
        if question.lower() in ['é€€å‡º', 'exit', 'quit']:
            break
        
        stream = input("æ˜¯å¦ä½¿ç”¨æµå¼å“åº”?(y/n): ").lower() == 'y'
        builder.query(qa_chain, question, stream)

if __name__ == "__main__":
    main()